{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A semantic approach for text clustering using WordNet and lexical chains.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10mb-rFx2DjSuOBDcIYpRBToL67x6agbH",
      "authorship_tag": "ABX9TyOu1+Qw4zoCuyGp2n/C8++Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamzakhan942/A-semantic-approach-for-text-clustering-using-WordNet-and-lexical-chains/blob/main/A_semantic_approach_for_text_clustering_using_WordNet_and_lexical_chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AolbF1W7N-jE"
      },
      "source": [
        "###A semantic approach for text clustering using WordNet and lexical chains\n",
        "## **http://dx.doi.org/10.1016/j.eswa.2014.10.023**\n",
        "Traditional clustering algorithms do not consider the semantic relationships among words so that cannot\n",
        "accurately represent the meaning of documents. To overcome this problem, introducing semantic information from ontology such as WordNet has been widely used to improve the quality of text clustering.\n",
        "However, there still exist several challenges, such as synonym and polysemy, high dimensionality,\n",
        "extracting core semantics from texts, and assigning appropriate description for the generated clusters.\n",
        "In this paper, we report our attempt towards integrating WordNet with lexical chains to alleviate these\n",
        "problems. The proposed approach exploits ontology hierarchical structure and relations to provide a\n",
        "more accurate assessment of the similarity between terms for word sense disambiguation. Furthermore,\n",
        "we introduce lexical chains to extract a set of semantically related words from texts, which can represent\n",
        "the semantic content of the texts. Although lexical chains have been extensively used in text summarization, their potential impact on text clustering problem has not been fully investigated. Our integrated\n",
        "way can identify the theme of documents based on the disambiguated core features extracted, and in parallel downsize the dimensions of feature space. The experimental results using the proposed framework\n",
        "on reuters-21578 show that clustering performance improves significantly compared to several classical\n",
        "methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xi_ddMtcBN0"
      },
      "source": [
        "###Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFnWwc5UNt3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5431bda6-b8cf-43cc-bed5-3330fc0ae738"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize \n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.metrics.cluster import homogeneity_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics.cluster import completeness_score\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJw_Df9_X5er"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrIyTKDobQOr"
      },
      "source": [
        "###Sense Disambiguation Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDjgkZNM-vVd"
      },
      "source": [
        "\"\"\"\n",
        "Sift4\n",
        "-----\n",
        "\n",
        "As described in:\n",
        "\n",
        "    <http://siderite.blogspot.com/2014/11/super-fast-and-accurate-string-distance.html>\n",
        "\n",
        "Sift4 is an approximation of Levenshtein distance,\n",
        "with O(n) complexity (whereas Levensthein is O(n*m)).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def sift4(s1, s2, max_offset=5):\n",
        "    \"\"\"\n",
        "    This is an implementation of general Sift4.\n",
        "    \"\"\"\n",
        "    t1, t2 = list(s1), list(s2)\n",
        "    l1, l2 = len(t1), len(t2)\n",
        "\n",
        "    if not s1:\n",
        "        return l2\n",
        "\n",
        "    if not s2:\n",
        "        return l1\n",
        "\n",
        "    # Cursors for each string\n",
        "    c1, c2 = 0, 0\n",
        "\n",
        "    # Largest common subsequence\n",
        "    lcss = 0\n",
        "\n",
        "    # Local common substring\n",
        "    local_cs = 0\n",
        "\n",
        "    # Number of transpositions ('ab' vs 'ba')\n",
        "    trans = 0\n",
        "\n",
        "    # Offset pair array, for computing the transpositions\n",
        "    offsets = []\n",
        "\n",
        "    while c1 < l1 and c2 < l2:\n",
        "        if t1[c1] == t2[c2]:\n",
        "            local_cs += 1\n",
        "\n",
        "            # Check if current match is a transposition\n",
        "            is_trans = False\n",
        "            i = 0\n",
        "            while i < len(offsets):\n",
        "                ofs = offsets[i]\n",
        "                if c1 <= ofs['c1'] or c2 <= ofs['c2']:\n",
        "                    is_trans = abs(c2-c1) >= abs(ofs['c2'] - ofs['c1'])\n",
        "                    if is_trans:\n",
        "                        trans += 1\n",
        "                    elif not ofs['trans']:\n",
        "                        ofs['trans'] = True\n",
        "                        trans += 1\n",
        "                    break\n",
        "                elif c1 > ofs['c2'] and c2 > ofs['c1']:\n",
        "                    del offsets[i]\n",
        "                else:\n",
        "                    i += 1\n",
        "            offsets.append({\n",
        "                'c1': c1,\n",
        "                'c2': c2,\n",
        "                'trans': is_trans\n",
        "            })\n",
        "\n",
        "        else:\n",
        "            lcss += local_cs\n",
        "            local_cs = 0\n",
        "            if c1 != c2:\n",
        "                c1 = c2 = min(c1, c2)\n",
        "\n",
        "            for i in range(max_offset):\n",
        "                if c1 + i >= l1 and c2 + i >= l2:\n",
        "                    break\n",
        "                elif c1 + i < l1 and s1[c1+i] == s2[c2]:\n",
        "                    c1 += i - 1\n",
        "                    c2 -= 1\n",
        "                    break\n",
        "\n",
        "                elif c2 + i < l2 and s1[c1] == s2[c2 + i]:\n",
        "                    c2 += i - 1\n",
        "                    c1 -= 1\n",
        "                    break\n",
        "\n",
        "        c1 += 1\n",
        "        c2 += 1\n",
        "\n",
        "        if c1 >= l1 or c2 >= l2:\n",
        "            lcss += local_cs\n",
        "            local_cs = 0\n",
        "            c1 = c2 = min(c1, c2)\n",
        "\n",
        "    lcss += local_cs\n",
        "    return round(max(l1, l2) - lcss + trans)\n",
        "\n",
        "\n",
        "def penn_to_wordnet(tag):\n",
        "    \"\"\"\n",
        "    Convert a Penn Treebank PoS tag to WordNet PoS tag.\n",
        "    \"\"\"\n",
        "    if tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
        "        return 'n' #wordnet.NOUN\n",
        "    elif tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
        "        return 'v' #wordnet.VERB\n",
        "    elif tag in ['RB', 'RBR', 'RBS']:\n",
        "        return 'r' #wordnet.ADV\n",
        "    elif tag in ['JJ', 'JJR', 'JJS']:\n",
        "        return 'a' #wordnet.ADJ\n",
        "    return None\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgcKpsbg_2dn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e97d0e-07c8-49e4-a91e-b2608887df5f"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H37PSqE1xl8w"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "import spacy\n",
        "# from broca.distance.sift4 import sift4\n",
        "# from broca.vectorize import Vectorizer\n",
        "# from broca.common.util import penn_to_wordnet\n",
        "# from broca.common.shared import spacy\n",
        "\n",
        "# from spacy.lang.en import English\n",
        "# spacy = English()\n",
        "\n",
        "stops = stop_words\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "class DCSVectorizer():\n",
        "    def __init__(self, alpha=1.5, relation_weights=[0.8, 0.5, 0.3], n_chains=100):\n",
        "        self.alpha = 1.5\n",
        "        self.relation_weights = relation_weights\n",
        "        self.n_chains = n_chains\n",
        "\n",
        "        # Cache concept => description\n",
        "        # and (c1, c2) => similarity\n",
        "        self.descriptions = {}\n",
        "        self.concept_sims = {}\n",
        "\n",
        "\n",
        "    def vectorize(self, docs):\n",
        "        \"\"\"\n",
        "        Vectorizes a list of documents using their DCS representations.\n",
        "        \"\"\"\n",
        "        doc_core_sems, all_concepts = self._extract_core_semantics(docs)\n",
        "\n",
        "        shape = (len(docs), len(all_concepts))\n",
        "        vecs = np.zeros(shape)\n",
        "        for i, core_sems in enumerate(doc_core_sems):\n",
        "            for con, weight in core_sems:\n",
        "                j = all_concepts.index(con)\n",
        "                vecs[i,j] = weight\n",
        "\n",
        "        # Normalize\n",
        "        return vecs/np.max(vecs)\n",
        "\n",
        "\n",
        "    def _process_doc(self, doc):\n",
        "        \"\"\"\n",
        "        Applies DCS to a document to extract its core concepts and their weights.\n",
        "        \"\"\"\n",
        "        # Prep\n",
        "        doc = doc.lower()\n",
        "        # nlp = spacy.load(\"en_core_web_sm\") (t, penn_to_wordnet(t.tag_)) spacy(doc, tag=True, parse=False, entity=False)\n",
        "        # nlp = spacy.load(\"en_core_web_sm\")\n",
        "        doc = nlp(doc)\n",
        "        tagged_tokens = [(t.text, penn_to_wordnet(t.tag_)) for t in doc]\n",
        "        tokens = [t for t, tag in tagged_tokens]\n",
        "        term_concept_map = self._disambiguate_doc(tagged_tokens)\n",
        "        concept_weights = self._weight_concepts(tokens, term_concept_map)\n",
        "\n",
        "        # Compute core semantics\n",
        "        lexical_chains = self._lexical_chains(doc, term_concept_map)\n",
        "        core_semantics = self._core_semantics(lexical_chains, concept_weights)\n",
        "        core_concepts = [c for chain in core_semantics for c in chain]\n",
        "\n",
        "        return [(con, concept_weights[con]) for con in core_concepts]\n",
        "\n",
        "\n",
        "    def _disambiguate_doc(self, tagged_tokens):\n",
        "        \"\"\"\n",
        "        Takes a list of tagged tokens, representing a document,\n",
        "        in the form:\n",
        "            [(token, tag), ...]\n",
        "        And returns a mapping of terms to their disambiguated concepts (synsets).\n",
        "        \"\"\"\n",
        "\n",
        "        # Group tokens by PoS\n",
        "        pos_groups = {pos: [] for pos in [wn.NOUN, wn.VERB, wn.ADJ, wn.ADV]}\n",
        "        for tok, tag in tagged_tokens:\n",
        "            if tag in pos_groups:\n",
        "                pos_groups[tag].append(tok)\n",
        "\n",
        "        #print(pos_groups)\n",
        "\n",
        "        # Map of final term -> concept mappings\n",
        "        map = {}\n",
        "        for tag, toks in pos_groups.items():\n",
        "            map.update(self._disambiguate_pos(toks, tag))\n",
        "\n",
        "        #nice_map = {k: map[k].lemma_names() for k in map.keys()}\n",
        "        #print(json.dumps(nice_map, indent=4, sort_keys=True))\n",
        "\n",
        "        return map\n",
        "\n",
        "\n",
        "    def _disambiguate_pos(self, terms, pos):\n",
        "        \"\"\"\n",
        "        Disambiguates a list of tokens of a given PoS.\n",
        "        \"\"\"\n",
        "        # Map the terms to candidate concepts\n",
        "        # Consider only the top 3 most common senses\n",
        "        candidate_map = {term: wn.synsets(term, pos=pos)[:3] for term in terms}\n",
        "\n",
        "        # Filter to unique concepts\n",
        "        concepts = set(c for cons in candidate_map.values() for c in cons)\n",
        "\n",
        "        # Back to list for consistent ordering\n",
        "        concepts = list(concepts)\n",
        "        sim_mat = self._similarity_matrix(concepts)\n",
        "\n",
        "        # Final map of terms to their disambiguated concepts\n",
        "        map = {}\n",
        "\n",
        "        # This is terrible\n",
        "        # For each term, select the candidate concept\n",
        "        # which has the maximum aggregate similarity score against\n",
        "        # all other candidate concepts of all other terms sharing the same PoS\n",
        "        for term, cons in candidate_map.items():\n",
        "            # Some words may not be in WordNet\n",
        "            # and thus have no candidate concepts, so skip\n",
        "            if not cons:\n",
        "                continue\n",
        "            scores = []\n",
        "            for con in cons:\n",
        "                i = concepts.index(con)\n",
        "                scores_ = []\n",
        "                for term_, cons_ in candidate_map.items():\n",
        "                    # Some words may not be in WordNet\n",
        "                    # and thus have no candidate concepts, so skip\n",
        "                    if term == term_ or not cons_:\n",
        "                        continue\n",
        "                    cons_idx = [concepts.index(c) for c in cons_]\n",
        "                    top_sim = max(sim_mat[i,cons_idx])\n",
        "                    scores_.append(top_sim)\n",
        "                scores.append(sum(scores_))\n",
        "            best_idx = np.argmax(scores)\n",
        "            map[term] = cons[best_idx]\n",
        "\n",
        "        return map\n",
        "\n",
        "\n",
        "    def _similarity_matrix(self, concepts):\n",
        "        \"\"\"\n",
        "        Computes a semantic similarity matrix for a set of concepts.\n",
        "        \"\"\"\n",
        "        n_cons = len(concepts)\n",
        "        sim_mat = np.zeros((n_cons, n_cons))\n",
        "        for i, c1 in enumerate(concepts):\n",
        "            for j, c2 in enumerate(concepts):\n",
        "                # Just build the lower triangle\n",
        "                if i >= j:\n",
        "                    sim_mat[i,j] = self._semsim(c1, c2) if i != j else 1.\n",
        "        return sim_mat + sim_mat.T - np.diag(sim_mat.diagonal())\n",
        "\n",
        "\n",
        "    def _semsim(self, c1, c2):\n",
        "        \"\"\"\n",
        "        Computes the semantic similarity between two concepts.\n",
        "        The semantic similarity is a combination of two sem sims:\n",
        "            1. An \"explicit\" sem sim metric, that is, one which is directly\n",
        "            encoded in the WordNet graph. Here it is just Wu-Palmer similarity.\n",
        "            2. An \"implicit\" sem sim metric. See `_imp_semsim`.\n",
        "        Note we can't use the NLTK Wu-Palmer similarity implementation because we need to\n",
        "        incorporate the implicit sem sim, but it's fairly straightforward --\n",
        "        leaning on <http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html#Synset.wup_similarity>,\n",
        "        see that for more info. Though...the formula in the paper includes an extra term in the denominator,\n",
        "        which is wrong, so we leave it out.\n",
        "        \"\"\"\n",
        "        if c1 == c2:\n",
        "            return 1.\n",
        "\n",
        "        if (c1, c2) in self.concept_sims:\n",
        "            return self.concept_sims[(c1, c2)]\n",
        "\n",
        "        elif (c2, c1) in self.concept_sims:\n",
        "            return self.concept_sims[(c2, c1)]\n",
        "\n",
        "        else:\n",
        "            need_root = c1._needs_root()\n",
        "            subsumers = c1.lowest_common_hypernyms(c2, simulate_root=need_root)\n",
        "\n",
        "            if not subsumers:\n",
        "                # For relationships not in WordNet, fallback on just implicit semsim.\n",
        "                return self._imp_semsim(c1, c2)\n",
        "\n",
        "            subsumer = subsumers[0]\n",
        "            depth = subsumer.max_depth() + 1\n",
        "            len1 = c1.shortest_path_distance(subsumer, simulate_root=need_root)\n",
        "            len2 = c2.shortest_path_distance(subsumer, simulate_root=need_root)\n",
        "\n",
        "            if len1 is None or len2 is None:\n",
        "                # See above\n",
        "                return self._imp_semsim(c1, c2)\n",
        "\n",
        "            len1 += depth\n",
        "            len2 += depth\n",
        "\n",
        "            imp_score = self._imp_semsim(c1, c2)\n",
        "\n",
        "            sim = (2.*depth + imp_score)/(len1 + len2 + imp_score)\n",
        "            self.concept_sims[(c1, c2)] = sim\n",
        "            return sim\n",
        "\n",
        "\n",
        "    def _imp_semsim(self, c1, c2):\n",
        "        \"\"\"\n",
        "        The paper's implicit semantic similarity metric\n",
        "        involves iteratively computing string overlaps;\n",
        "        this is a modification where we instead use\n",
        "        inverse Sift4 distance (a fast approximation of Levenshtein distance).\n",
        "        Frankly ~ I don't know if this is an appropriate\n",
        "        substitute, so I'll have to play around with this and see.\n",
        "        \"\"\"\n",
        "\n",
        "        desc1 = self._description(c1)\n",
        "        desc2 = self._description(c2)\n",
        "\n",
        "        raw_sim = 1/(sift4(desc1, desc2) + 1)\n",
        "        return math.log(raw_sim + 1)\n",
        "\n",
        "\n",
        "    def _core_semantics(self, lex_chains, concept_weights):\n",
        "        \"\"\"\n",
        "        Returns the n representative lexical chains for a document.\n",
        "        \"\"\"\n",
        "        chain_scores = [self._score_chain(lex_chain, adj_submat, concept_weights) for lex_chain, adj_submat in lex_chains]\n",
        "        scored_chains = zip(lex_chains, chain_scores)\n",
        "        scored_chains = sorted(scored_chains, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        thresh = (self.alpha/len(lex_chains)) * sum(chain_scores)\n",
        "        return [chain for (chain, adj_mat), score in scored_chains if score >= thresh][:self.n_chains]\n",
        "\n",
        "\n",
        "    def _extract_core_semantics(self, docs):\n",
        "        \"\"\"\n",
        "        Extracts core semantics for a list of documents, returning them along with\n",
        "        a list of all the concepts represented.\n",
        "        \"\"\"\n",
        "        all_concepts = []\n",
        "        doc_core_sems = []\n",
        "        for doc in docs:\n",
        "            core_sems = self._process_doc(doc)\n",
        "            doc_core_sems.append(core_sems)\n",
        "            all_concepts += [con for con, weight in core_sems]\n",
        "        return doc_core_sems, list(set(all_concepts))\n",
        "\n",
        "\n",
        "    def _lexical_chains(self, doc, term_concept_map):\n",
        "        \"\"\"\n",
        "        Builds lexical chains, as an adjacency matrix,\n",
        "        using a disambiguated term-concept map.\n",
        "        \"\"\"\n",
        "        concepts = list({c for c in term_concept_map.values()})\n",
        "\n",
        "        # Build an adjacency matrix for the graph\n",
        "        # Using the encoding:\n",
        "        # 1 = identity/synonymy, 2 = hypernymy/hyponymy, 3 = meronymy, 0 = no edge\n",
        "        n_cons = len(concepts)\n",
        "        adj_mat = np.zeros((n_cons, n_cons))\n",
        "\n",
        "        for i, c in enumerate(concepts):\n",
        "            # TO DO can only do i >= j since the graph is undirected\n",
        "            for j, c_ in enumerate(concepts):\n",
        "                edge = 0\n",
        "                if c == c_:\n",
        "                    edge = 1\n",
        "                # TO DO when should simulate root be True?\n",
        "                elif c_ in c._shortest_hypernym_paths(simulate_root=False).keys():\n",
        "                    edge = 2\n",
        "                elif c in c_._shortest_hypernym_paths(simulate_root=False).keys():\n",
        "                    edge = 2\n",
        "                elif c_ in c.member_meronyms() + c.part_meronyms() + c.substance_meronyms():\n",
        "                    edge = 3\n",
        "                elif c in c_.member_meronyms() + c_.part_meronyms() + c_.substance_meronyms():\n",
        "                    edge = 3\n",
        "\n",
        "                adj_mat[i,j] = edge\n",
        "\n",
        "        # Group connected concepts by labels\n",
        "        concept_labels = connected_components(adj_mat, directed=False)[1]\n",
        "        lexical_chains = [([], []) for i in range(max(concept_labels) + 1)]\n",
        "        for i, concept in enumerate(concepts):\n",
        "            label = concept_labels[i]\n",
        "            lexical_chains[label][0].append(concept)\n",
        "            lexical_chains[label][1].append(i)\n",
        "\n",
        "        # Return the lexical chains as (concept list, adjacency sub-matrix) tuples\n",
        "        return [(chain, adj_mat[indices][:,indices]) for chain, indices in lexical_chains]\n",
        "\n",
        "\n",
        "    def _score_chain(self, lexical_chain, adj_submat, concept_weights):\n",
        "        \"\"\"\n",
        "        Computes the score for a lexical chain.\n",
        "        \"\"\"\n",
        "        scores = []\n",
        "\n",
        "        # Compute scores for concepts in the chain\n",
        "        for i, c in enumerate(lexical_chain):\n",
        "            score = concept_weights[c] * self.relation_weights[0]\n",
        "            rel_scores = []\n",
        "            for j, c_ in enumerate(lexical_chain):\n",
        "                if adj_submat[i,j] == 2:\n",
        "                    rel_scores.append(self.relation_weights[1] * concept_weights[c_])\n",
        "\n",
        "                elif adj_submat[i,j] == 3:\n",
        "                    rel_scores.append(self.relation_weights[2] * concept_weights[c_])\n",
        "\n",
        "            scores.append(score + sum(rel_scores))\n",
        "\n",
        "        # The chain's score is just the sum of its concepts' scores\n",
        "        return sum(scores)\n",
        "\n",
        "\n",
        "    def _weight_concepts(self, tokens, term_concept_map):\n",
        "        \"\"\"\n",
        "        Calculates weights for concepts in a document.\n",
        "        This is just the frequency of terms which map to a concept.\n",
        "        \"\"\"\n",
        "\n",
        "        weights = {c: 0 for c in term_concept_map.values()}\n",
        "        for t in tokens:\n",
        "            # Skip terms that aren't one of the PoS we used\n",
        "            if t not in term_concept_map:\n",
        "                continue\n",
        "            con = term_concept_map[t]\n",
        "            weights[con] += 1\n",
        "\n",
        "        # TO DO paper doesn't mention normalizing these weights...should we?\n",
        "        return weights\n",
        "\n",
        "\n",
        "    def _description(self, concept):\n",
        "        \"\"\"\n",
        "        Returns a \"description\" of a concept,\n",
        "        as defined in the paper.\n",
        "        The paper describes the description as a string,\n",
        "        so this is a slight modification where we instead represent\n",
        "        the definition as a list of tokens.\n",
        "        \"\"\"\n",
        "        if concept not in self.descriptions:\n",
        "            lemmas = concept.lemma_names()\n",
        "            gloss = self._gloss(concept)\n",
        "            glosses = [self._gloss(rel) for rel in self._related(concept)]\n",
        "            raw_desc = ' '.join(lemmas + [gloss] + glosses)\n",
        "            desc = [w for w in raw_desc.split() if w not in stops]\n",
        "            self.descriptions[concept] = desc\n",
        "        return self.descriptions[concept]\n",
        "\n",
        "\n",
        "    def _gloss(self, concept):\n",
        "        \"\"\"\n",
        "        The concatenation of a concept's definition and its examples.\n",
        "        \"\"\"\n",
        "        return  ' '.join([concept.definition()] + concept.examples())\n",
        "\n",
        "\n",
        "    def _related(self, concept):\n",
        "        \"\"\"\n",
        "        Returns related concepts for a concept.\n",
        "        \"\"\"\n",
        "        return concept.hypernyms() + \\\n",
        "                concept.hyponyms() + \\\n",
        "                concept.member_meronyms() + \\\n",
        "                concept.substance_meronyms() + \\\n",
        "                concept.part_meronyms() + \\\n",
        "                concept.member_holonyms() + \\\n",
        "                concept.substance_holonyms() + \\\n",
        "                concept.part_holonyms() + \\\n",
        "                concept.attributes() + \\\n",
        "                concept.also_sees() + \\\n",
        "                concept.similar_tos()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6AoJhazbW0d"
      },
      "source": [
        "###Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzjEZKqJIPFi"
      },
      "source": [
        "# import os\n",
        "# DATA_SET_PATH = '/content/drive/MyDrive/FYP-1-DLS-Clustering /DataSets/Reuters-DataSets/'\n",
        "# dataset_files = os.listdir(DATA_SET_PATH+'Reuters')\n",
        "# categories = os.listdir(DATA_SET_PATH+'Reuters GT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-E4uAXXcOjs"
      },
      "source": [
        "Mapping classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap-u8iMVSctk"
      },
      "source": [
        "# cat_map = {}\n",
        "\n",
        "# for category in categories:\n",
        "#   cat_map[category] = os.listdir(DATA_SET_PATH+'Reuters GT/'+category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRa4pSnzcRlI"
      },
      "source": [
        "creating corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMXb6bWLT0BH"
      },
      "source": [
        "# corpus = []\n",
        "# for file_name in dataset_files:\n",
        "#   text = open(DATA_SET_PATH+'Reuters/'+file_name, 'r')\n",
        "#   text = text.read()\n",
        "#   corpus.append(text)\n",
        "\n",
        "# corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lMktxLocT99"
      },
      "source": [
        "custom encoder-decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQDdPxTVcdMV"
      },
      "source": [
        "# encoder = {}\n",
        "# decoder = {}\n",
        "\n",
        "# for code, cat in enumerate(cat_map.keys()):\n",
        "#   encoder[cat] = code\n",
        "#   decoder[code] = cat\n",
        "\n",
        "# encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwv_wIl9cYAl"
      },
      "source": [
        "###Labelling Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJIeuz2mdH2P"
      },
      "source": [
        "# labels = []\n",
        "# decoded_cats = []\n",
        "# seen = set()\n",
        "# for file_name in dataset_files:\n",
        "#   for key in cat_map.keys():\n",
        "#     if file_name in cat_map[key] and file_name not in seen:\n",
        "#       seen.add(file_name)\n",
        "#       labels.append(encoder[key])\n",
        "#       decoded_cats.append(key)\n",
        "\n",
        "# len(labels)\n",
        "# decoded_cats "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C6ALWNRYVLB"
      },
      "source": [
        "# id = []\n",
        "# doc = []\n",
        "# label = []\n",
        "# for sentence in corpus:\n",
        "#   id.append(sentence.split('ID: ')[1].split(' ')[0])\n",
        "#   if len(sentence.split('TEXT: ')) > 1:\n",
        "#     doc.append(sentence.split('TEXT: ')[1])\n",
        "#   else:\n",
        "#     doc.append(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Powvmgloce1A"
      },
      "source": [
        "saving Dataset as compressed pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFFTltB_aTva"
      },
      "source": [
        "# import pandas as pd\n",
        "# data_frame = pd.DataFrame()\n",
        "# data_frame['id'] = id\n",
        "# data_frame['file_name'] = dataset_files\n",
        "# data_frame['category'] = decoded_cats\n",
        "# data_frame['label'] = labels \n",
        "# data_frame['doc'] = doc\n",
        "# data_frame.to_pickle('/content/drive/MyDrive/FYP-1-DLS-Clustering /DataSets/reuters_pickle.pk1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "ViEqbYKEckqN",
        "outputId": "7ec9bbc4-47ac-4223-ca90-201f0f991597"
      },
      "source": [
        "# data_frame"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>file_name</th>\n",
              "      <th>category</th>\n",
              "      <th>label</th>\n",
              "      <th>doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9958</td>\n",
              "      <td>21531.txt</td>\n",
              "      <td>barley</td>\n",
              "      <td>11</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17379</td>\n",
              "      <td>8200.txt</td>\n",
              "      <td>ipi</td>\n",
              "      <td>0</td>\n",
              "      <td>China's total wage bill for state employees gr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>17424</td>\n",
              "      <td>8251.txt</td>\n",
              "      <td>carcass</td>\n",
              "      <td>3</td>\n",
              "      <td>Members of local 174 of the United Food and Co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17380</td>\n",
              "      <td>8202.txt</td>\n",
              "      <td>barley</td>\n",
              "      <td>11</td>\n",
              "      <td>West German use of tapioca is likely to declin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17303</td>\n",
              "      <td>8117.txt</td>\n",
              "      <td>ipi</td>\n",
              "      <td>0</td>\n",
              "      <td>Japan's preliminary industrial production inde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>17387</td>\n",
              "      <td>8209.txt</td>\n",
              "      <td>iron-steel</td>\n",
              "      <td>2</td>\n",
              "      <td>A Greek bulk carrier loaded with iron ore has ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782</th>\n",
              "      <td>17384</td>\n",
              "      <td>8206.txt</td>\n",
              "      <td>jobs</td>\n",
              "      <td>5</td>\n",
              "      <td>Unemployment in the European Community fell in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>783</th>\n",
              "      <td>17312</td>\n",
              "      <td>8127.txt</td>\n",
              "      <td>iron-steel</td>\n",
              "      <td>2</td>\n",
              "      <td>South Korea unveiled a shopping list of 2.6 bi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>784</th>\n",
              "      <td>17315</td>\n",
              "      <td>8130.txt</td>\n",
              "      <td>iron-steel</td>\n",
              "      <td>2</td>\n",
              "      <td>Stelco Inc said contract negotiations with Uni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785</th>\n",
              "      <td>17455</td>\n",
              "      <td>8285.txt</td>\n",
              "      <td>rubber</td>\n",
              "      <td>9</td>\n",
              "      <td>The Tokyo Commodity Exchange for Industry (TOC...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>786 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  file_name  ... label                                                doc\n",
              "0     9958  21531.txt  ...    11                                                   \n",
              "1    17379   8200.txt  ...     0  China's total wage bill for state employees gr...\n",
              "2    17424   8251.txt  ...     3  Members of local 174 of the United Food and Co...\n",
              "3    17380   8202.txt  ...    11  West German use of tapioca is likely to declin...\n",
              "4    17303   8117.txt  ...     0  Japan's preliminary industrial production inde...\n",
              "..     ...        ...  ...   ...                                                ...\n",
              "781  17387   8209.txt  ...     2  A Greek bulk carrier loaded with iron ore has ...\n",
              "782  17384   8206.txt  ...     5  Unemployment in the European Community fell in...\n",
              "783  17312   8127.txt  ...     2  South Korea unveiled a shopping list of 2.6 bi...\n",
              "784  17315   8130.txt  ...     2  Stelco Inc said contract negotiations with Uni...\n",
              "785  17455   8285.txt  ...     9  The Tokyo Commodity Exchange for Industry (TOC...\n",
              "\n",
              "[786 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 401
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvHWDerfcnvQ"
      },
      "source": [
        "###Loading saved pickle data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1qNDO24bt_0"
      },
      "source": [
        "import pandas as pd\n",
        "unpickled_df = pd.read_pickle(\"/content/drive/MyDrive/FYP-1-DLS-Clustering /DataSets/reuters_pickle.pk1\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JlWJKKBkimV"
      },
      "source": [
        "corpus_unpkled = unpickled_df['doc'].tolist()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XczVZNUcsrn"
      },
      "source": [
        "filteration step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE2EzlrLlP2Q"
      },
      "source": [
        "corpus_filtered = []\n",
        "labels_filterted = unpickled_df['label'].tolist()\n",
        "for index, sentence in enumerate(corpus_unpkled):\n",
        "  if len(sentence) > 1:\n",
        "    corpus_filtered.append(sentence)\n",
        "  else:\n",
        "    labels_filterted.pop(index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2cwBS0pbdL5"
      },
      "source": [
        "###Corpus Disambiguation - no preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz9PEr6Ik1FS"
      },
      "source": [
        "disamb = DCSVectorizer()\n",
        "results = disamb.vectorize(corpus_filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we6b41eRqD0G",
        "outputId": "0541b606-b485-414b-aac5-5b587dbc316a"
      },
      "source": [
        "results.shape "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(725, 2071)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYA_1HYYqLSS"
      },
      "source": [
        "np.save('/content/drive/MyDrive/FYP-1-DLS-Clustering /DataSets/reuters_results.npy', results)\n",
        "# np.save('/content/drive/MyDrive/FYP-1-DLS-Clustering /DataSets/reuters_50_lex_results.npy', results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jyU88Rjbk0u"
      },
      "source": [
        "###1. Saved Results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1tcXtEYqjYM"
      },
      "source": [
        "X = np.load('/content/drive/MyDrive/FYP-1-DLS-Clustering /DataSets/reuters_50_lex_results.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjJh1yPlq0G7"
      },
      "source": [
        "kmeans = KMeans(n_clusters=15, random_state=135).fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNdKWuu-5a0U"
      },
      "source": [
        "clusts = kmeans.labels_.tolist()\n",
        "result_frame = pd.DataFrame()\n",
        "result_frame['True Label'] = labels_filterted\n",
        "result_frame['Predicted Labels'] = clusts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx3g-aTSahvd",
        "outputId": "b26513f1-52b6-485b-c8d6-d396f875b4b1"
      },
      "source": [
        "f_score = f1_score(labels_filterted, clusts, average='micro') \n",
        "purity = homogeneity_score(labels_filterted, clusts) \n",
        "completeness = completeness_score(labels_filterted, clusts)\n",
        "print('=========================================')\n",
        "print(f'| F Score\\t| {f_score}\\t|\\n| Purity\\t| {purity}\\t|\\n| Completeness\\t| {completeness}\\t|')\n",
        "print('=========================================')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================\n",
            "| F Score\t| 0.06344827586206897\t|\n",
            "| Purity\t| 0.04042845972406231\t|\n",
            "| Completeness\t| 0.06398373825269524\t|\n",
            "=========================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNETxytnbq8a"
      },
      "source": [
        "###Corpus Disambiguation - Prepocessed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt7ywRbDJghr"
      },
      "source": [
        "def pre_processor(docs):\n",
        "  processed_text = []\n",
        "  ps = PorterStemmer()\n",
        "  lm = WordNetLemmatizer() \n",
        "  for sent in docs:\n",
        "      sent = sent.lower()\n",
        "      sent = word_tokenize(sent)\n",
        "      sent = [word for word in sent if word not in stop_words]\n",
        "      sent = [clean for clean in sent if clean.isalpha()]  # removal of special characters\n",
        "      sent = [lm.lemmatize(lem) for lem in sent]\n",
        "      sent = [ps.stem(stem) for stem in sent]\n",
        "      sent = ' '.join(sent)\n",
        "      processed_text.append(sent)\n",
        "\n",
        "  return processed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvItfpzmI_yX"
      },
      "source": [
        "unpickled_two = pd.read_pickle(\"/content/drive/MyDrive/FYP-1-DLS-Clustering /DataSets/reuters_pickle.pk1\")\n",
        "stem_lem_corpus =  unpickled_two['doc'].tolist()\n",
        "stem_lem_corpus = pre_processor(stem_lem_corpus) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoPnTHG4KePq"
      },
      "source": [
        "new_corpus_filtered = []\n",
        "new_labels_filterted = unpickled_two['label'].tolist()\n",
        "for index, sentence in enumerate(stem_lem_corpus):\n",
        "  if len(sentence) > 1:\n",
        "    new_corpus_filtered.append(sentence)\n",
        "  else:\n",
        "    new_labels_filterted.pop(index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nj_3jK8O5kh"
      },
      "source": [
        "disamb = DCSVectorizer()\n",
        "new_results = disamb.vectorize(new_corpus_filtered) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re0-iL42QkPB",
        "outputId": "6406e422-e05a-4023-f19d-5c256279f36b"
      },
      "source": [
        "new_results.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(725, 1351)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeOIPNyJQuPp"
      },
      "source": [
        "# np.save('/content/drive/MyDrive/FYP-1-DLS-Clustering /DataSets/new_reuters_results.npy', new_results)\n",
        "np.save('/content/drive/MyDrive/FYP-1-DLS-Clustering /DataSets/new_reuters_100_lex_results.npy', new_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tufSCoZbx4C"
      },
      "source": [
        "###2. Saved Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVWssmbNQyxu"
      },
      "source": [
        "X2 = np.load('/content/drive/MyDrive/FYP-1-DLS-Clustering /DataSets/new_reuters_100_lex_results.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ejADl56RHF6",
        "outputId": "691be7ad-1b70-498d-c882-f0317e9653d0"
      },
      "source": [
        "new_kmeans = KMeans(n_clusters=15, random_state=135).fit(X)\n",
        "new_clusts = new_kmeans.labels_.tolist()\n",
        "purity = homogeneity_score(new_labels_filterted, new_clusts)\n",
        "f_score = f1_score(new_labels_filterted, new_clusts, average='micro') \n",
        "completeness = completeness_score(new_labels_filterted, new_clusts)\n",
        "print('=========================================')\n",
        "print(f'| F Score\\t| {f_score}\\t|\\n| Purity\\t| {purity}\\t|\\n| Completeness\\t| {completeness}\\t|')\n",
        "print('=========================================') \n",
        "analysis_results = [f_score, purity, completeness]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================\n",
            "| F Score\t| 0.06344827586206897\t|\n",
            "| Purity\t| 0.04042845972406231\t|\n",
            "| Completeness\t| 0.06398373825269524\t|\n",
            "=========================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TivhYMZOb2oj"
      },
      "source": [
        "###Tuned Results for max purity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgNTnsV0YuEY",
        "outputId": "af20a4e6-60e1-41bb-f9d2-01b3cddbd512"
      },
      "source": [
        "new_kmeans = KMeans(n_clusters=97, random_state=34).fit(X)\n",
        "new_clusts = new_kmeans.labels_.tolist()\n",
        "purity = homogeneity_score(new_labels_filterted, new_clusts)\n",
        "f_score = f1_score(new_labels_filterted, new_clusts, average='micro') \n",
        "completeness = completeness_score(new_labels_filterted, new_clusts)\n",
        "print('=========================================')\n",
        "print(f'| F Score\\t| {f_score}\\t|\\n| Purity\\t| {purity}\\t|\\n| Completeness\\t| {completeness}\\t|')\n",
        "print('=========================================')\n",
        "tuned_purity = [f_score, purity, completeness]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================\n",
            "| F Score\t| 0.009655172413793104\t|\n",
            "| Purity\t| 0.20283730361969843\t|\n",
            "| Completeness\t| 0.16146775772465602\t|\n",
            "=========================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6kmnRr855Cj"
      },
      "source": [
        "###Comparison Study"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "M34kku_O54rm",
        "outputId": "3268772b-f917-4b29-e0e0-b58973490a1e"
      },
      "source": [
        "dat = pd.read_pickle('/content/drive/MyDrive/FYP-1-DLS-Clustering /DataSets/graphing_data.pkl') \n",
        "dat['dcs'] = analysis_results\n",
        "dat['tuned_purity'] = tuned_purity\n",
        "dat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tf-idf</th>\n",
              "      <th>n-gram</th>\n",
              "      <th>dcs</th>\n",
              "      <th>tuned_purity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>F Score</th>\n",
              "      <td>0.060690</td>\n",
              "      <td>0.067586</td>\n",
              "      <td>0.111724</td>\n",
              "      <td>0.009655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Purity</th>\n",
              "      <td>0.068473</td>\n",
              "      <td>0.064117</td>\n",
              "      <td>0.041810</td>\n",
              "      <td>0.202837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Completeness</th>\n",
              "      <td>0.065251</td>\n",
              "      <td>0.066043</td>\n",
              "      <td>0.071828</td>\n",
              "      <td>0.161468</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                tf-idf    n-gram       dcs  tuned_purity\n",
              "F Score       0.060690  0.067586  0.111724      0.009655\n",
              "Purity        0.068473  0.064117  0.041810      0.202837\n",
              "Completeness  0.065251  0.066043  0.071828      0.161468"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 415
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axFUAwYv6hIJ"
      },
      "source": [
        "Graphing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "yIixv5Mr6jy7",
        "outputId": "7fa002ae-7108-4a9a-9987-de4308143b9e"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "  x = [['F Score', 'Purity', 'Completeness'],\n",
        "       [\"TF IDF\", \"TF IDF\", \"TF IDF\"]],\n",
        "  y = [0.06069, 0.068473, 0.065251],\n",
        "  name = \"TF IDF\",\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "  x = [['F Score', 'Purity', 'Completeness'],\n",
        "       [\"N GRAM\", \"N GRAM\", \"N GRAM\"]],\n",
        "  y = dat['n-gram'],\n",
        "  name = \"N GRAM\",\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "  x = [['F Score', 'Purity', 'Completeness'],\n",
        "       [\"Word Sense\", \"Word Sense\", \"Word Sense\"]],\n",
        "  y = dat['dcs'],\n",
        "  name = \"Word Sense\",\n",
        "))\n",
        "\n",
        "\n",
        "fig.update_layout(title_text=\"Multi-category axis\")\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"ac7beff3-e926-439d-a727-2c4c1875747d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"ac7beff3-e926-439d-a727-2c4c1875747d\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'ac7beff3-e926-439d-a727-2c4c1875747d',\n",
              "                        [{\"name\": \"TF IDF\", \"type\": \"bar\", \"x\": [[\"F Score\", \"Purity\", \"Completeness\"], [\"TF IDF\", \"TF IDF\", \"TF IDF\"]], \"y\": [0.06069, 0.068473, 0.065251]}, {\"name\": \"N GRAM\", \"type\": \"bar\", \"x\": [[\"F Score\", \"Purity\", \"Completeness\"], [\"N GRAM\", \"N GRAM\", \"N GRAM\"]], \"y\": [0.06758620689655172, 0.06411661949081643, 0.06604292598251892]}, {\"name\": \"Word Sense\", \"type\": \"bar\", \"x\": [[\"F Score\", \"Purity\", \"Completeness\"], [\"Word Sense\", \"Word Sense\", \"Word Sense\"]], \"y\": [0.11172413793103449, 0.04180956080250367, 0.07182750332740666]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Multi-category axis\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ac7beff3-e926-439d-a727-2c4c1875747d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQGz-Va8-4K3"
      },
      "source": [
        "Purity Tuned Resulted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "pK9RTFYr-1md",
        "outputId": "4f071577-a50c-4728-81bb-c9e96e079ab1"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "  x = [['F Score', 'Purity', 'Completeness'],\n",
        "       [\"TF IDF\", \"TF IDF\", \"TF IDF\"]],\n",
        "  y = [0.06069, 0.068473, 0.065251],\n",
        "  name = \"TF IDF\",\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "  x = [['F Score', 'Purity', 'Completeness'],\n",
        "       [\"N GRAM\", \"N GRAM\", \"N GRAM\"]],\n",
        "  y = dat['n-gram'],\n",
        "  name = \"N GRAM\",\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "  x = [['F Score', 'Purity', 'Completeness'],\n",
        "       [\"Word Sense\", \"Word Sense\", \"Word Sense\"]],\n",
        "  y = dat['dcs'],\n",
        "  name = \"Word Sense\",\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "  x = [['F Score', 'Purity', 'Completeness'],\n",
        "       [\"DCS Tuned\", \"DCS Tuned\", \"DCS Tuned\"]],\n",
        "  y = dat['tuned_purity'],\n",
        "  name = \"DCS Tuned\",\n",
        "))\n",
        "\n",
        "fig.update_layout(title_text=\"Multi-category axis\")\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"c8167b03-650a-428e-8011-3ad8f636998b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"c8167b03-650a-428e-8011-3ad8f636998b\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'c8167b03-650a-428e-8011-3ad8f636998b',\n",
              "                        [{\"name\": \"TF IDF\", \"type\": \"bar\", \"x\": [[\"F Score\", \"Purity\", \"Completeness\"], [\"TF IDF\", \"TF IDF\", \"TF IDF\"]], \"y\": [0.06069, 0.068473, 0.065251]}, {\"name\": \"N GRAM\", \"type\": \"bar\", \"x\": [[\"F Score\", \"Purity\", \"Completeness\"], [\"N GRAM\", \"N GRAM\", \"N GRAM\"]], \"y\": [0.06758620689655172, 0.06411661949081643, 0.06604292598251892]}, {\"name\": \"Word Sense\", \"type\": \"bar\", \"x\": [[\"F Score\", \"Purity\", \"Completeness\"], [\"Word Sense\", \"Word Sense\", \"Word Sense\"]], \"y\": [0.11172413793103449, 0.04180956080250367, 0.07182750332740666]}, {\"name\": \"DCS Tuned\", \"type\": \"bar\", \"x\": [[\"F Score\", \"Purity\", \"Completeness\"], [\"DCS Tuned\", \"DCS Tuned\", \"DCS Tuned\"]], \"y\": [0.009655172413793104, 0.20283730361969843, 0.16146775772465602]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Multi-category axis\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c8167b03-650a-428e-8011-3ad8f636998b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8h7HDLZZ5NF"
      },
      "source": [
        "###Logs of results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac2co527bGxh"
      },
      "source": [
        "\n",
        "seeed 32 - F Score: 0.08275862068965517 Purity: 0.074076717340167 - 29\n",
        "\n",
        "seeed 56 - F Score: 0.08275862068965517 Purity: 0.11232005623026102 - 45\n",
        "\n",
        "seeed 34 - F Score: 0.08275862068965517 Purity: 0.20283730361969843 - 97\n",
        "\n",
        "seeed 126 - F Score: 0.08275862068965517 Purity: 0.03270145809601008 - 10\n",
        "\n",
        "seeed 129 - F Score: 0.08275862068965517 Purity: 0.037093393568451406 - 11\n",
        "\n",
        "seeed 77 - F Score: 0.08275862068965517 Purity: 0.03721067754401603 - 12\n",
        "\n",
        "seeed 116 - F Score: 0.08275862068965517 Purity: 0.04066024768518942 - 13\n",
        "\n",
        "seeed 116 - F Score: 0.08275862068965517 Purity: 0.04197344729337054 - 14\n",
        "\n",
        "seeed 1 - F Score: 0.08275862068965517 Purity: 0.05088905270023905 - 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gody2-5NbE8G"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbtimEU4TUbO"
      },
      "source": [
        "# 29. -seeed 32- F Score: 0.08275862068965517 Purity: 0.074076717340167\n",
        "# 45. -seeed 56- F Score: 0.08275862068965517 Purity: 0.11232005623026102\n",
        "# 97. -seeed 34- F Score: 0.08275862068965517 Purity: 0.20283730361969843\n",
        "# 43. -seeed 126- F Score: 0.08275862068965517 Purity: 0.03270145809601008 - 10\n",
        "# 8. -seeed 129- F Score: 0.08275862068965517 Purity: 0.037093393568451406 - 11\n",
        "# 25. -seeed 77- F Score: 0.08275862068965517 Purity: 0.03721067754401603 - 12\n",
        "# 10. -seeed 116- F Score: 0.08275862068965517 Purity: 0.04066024768518942 - 13\n",
        "# 12. -seeed 116- F Score: 0.08275862068965517 Purity: 0.04197344729337054 - 14\n",
        "# 62. -seeed 1- F Score: 0.08275862068965517 Purity: 0.05088905270023905 - 15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrbG2JgQgQZP"
      },
      "source": [
        "Tuner "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i07ui16wgKul",
        "outputId": "8eb4de11-259f-4bc0-eba7-b29725dc60ac"
      },
      "source": [
        "import random\n",
        "max_f = 0\n",
        "max_p = 0\n",
        "mf = ''\n",
        "mp = ''\n",
        "for k in range(0, 100):\n",
        "  seed = random.randint(0, 150)\n",
        "  new_kmeans = KMeans(n_clusters=15, random_state=seed).fit(X)\n",
        "  new_clusts = new_kmeans.labels_.tolist()\n",
        "  purity = homogeneity_score(new_labels_filterted, new_clusts)\n",
        "  f_score = f1_score(new_labels_filterted, new_clusts, average='micro') \n",
        "  print(f'Seed: {seed}\\tF-Score:{f_score}\\tPurity: {purity}')\n",
        "  if f_score > max_f:\n",
        "    max_f = f_score\n",
        "    mf = f'Seed: {seed}\\tF-Score:{f_score}\\tPurity: {purity}'\n",
        "  if purity > max_p:\n",
        "    max_p = purity\n",
        "    mp = f'Seed: {seed}\\tF-Score:{f_score}\\tPurity: {purity}'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: 25\tF-Score:0.07724137931034483\tPurity: 0.03474481452610758\n",
            "Seed: 18\tF-Score:0.07172413793103448\tPurity: 0.04171561234620978\n",
            "Seed: 11\tF-Score:0.07172413793103448\tPurity: 0.03881990021073411\n",
            "Seed: 45\tF-Score:0.08551724137931034\tPurity: 0.03705000068618942\n",
            "Seed: 101\tF-Score:0.08\tPurity: 0.043447184444153235\n",
            "Seed: 99\tF-Score:0.08689655172413793\tPurity: 0.03901449397954723\n",
            "Seed: 135\tF-Score:0.11172413793103449\tPurity: 0.04180956080250367\n",
            "Seed: 57\tF-Score:0.08827586206896551\tPurity: 0.04047244318333482\n",
            "Seed: 22\tF-Score:0.05655172413793103\tPurity: 0.039489503788570156\n",
            "Seed: 91\tF-Score:0.08689655172413793\tPurity: 0.04069565653850308\n",
            "Seed: 92\tF-Score:0.06206896551724138\tPurity: 0.03794281309835939\n",
            "Seed: 68\tF-Score:0.02620689655172414\tPurity: 0.04122321376116574\n",
            "Seed: 145\tF-Score:0.07724137931034483\tPurity: 0.03999303826776661\n",
            "Seed: 3\tF-Score:0.08137931034482758\tPurity: 0.04168762594642023\n",
            "Seed: 75\tF-Score:0.07586206896551724\tPurity: 0.04006531622613011\n",
            "Seed: 84\tF-Score:0.08275862068965517\tPurity: 0.03677267846472613\n",
            "Seed: 36\tF-Score:0.08275862068965517\tPurity: 0.0380373293989639\n",
            "Seed: 47\tF-Score:0.07586206896551724\tPurity: 0.04433720768477484\n",
            "Seed: 110\tF-Score:0.04551724137931035\tPurity: 0.03715414510420421\n",
            "Seed: 13\tF-Score:0.07862068965517241\tPurity: 0.03641894179206096\n",
            "Seed: 69\tF-Score:0.04827586206896552\tPurity: 0.046297459269117006\n",
            "Seed: 18\tF-Score:0.07172413793103448\tPurity: 0.04171561234620978\n",
            "Seed: 85\tF-Score:0.0703448275862069\tPurity: 0.0331000278224226\n",
            "Seed: 65\tF-Score:0.07586206896551724\tPurity: 0.0488155766749684\n",
            "Seed: 37\tF-Score:0.0993103448275862\tPurity: 0.03611637276574603\n",
            "Seed: 127\tF-Score:0.08413793103448276\tPurity: 0.04222112055460975\n",
            "Seed: 36\tF-Score:0.08275862068965517\tPurity: 0.0380373293989639\n",
            "Seed: 110\tF-Score:0.04551724137931035\tPurity: 0.03715414510420421\n",
            "Seed: 18\tF-Score:0.07172413793103448\tPurity: 0.04171561234620978\n",
            "Seed: 105\tF-Score:0.07172413793103448\tPurity: 0.037155271438096306\n",
            "Seed: 129\tF-Score:0.057931034482758624\tPurity: 0.03838863913569056\n",
            "Seed: 120\tF-Score:0.05103448275862069\tPurity: 0.04267596008386681\n",
            "Seed: 139\tF-Score:0.04551724137931035\tPurity: 0.04572391912582836\n",
            "Seed: 77\tF-Score:0.05379310344827586\tPurity: 0.03996012219663795\n",
            "Seed: 93\tF-Score:0.07862068965517241\tPurity: 0.040505004332351956\n",
            "Seed: 75\tF-Score:0.07586206896551724\tPurity: 0.04006531622613011\n",
            "Seed: 117\tF-Score:0.08965517241379309\tPurity: 0.04001748551402167\n",
            "Seed: 117\tF-Score:0.08965517241379309\tPurity: 0.04001748551402167\n",
            "Seed: 101\tF-Score:0.08\tPurity: 0.043447184444153235\n",
            "Seed: 120\tF-Score:0.05103448275862069\tPurity: 0.04267596008386681\n",
            "Seed: 95\tF-Score:0.07586206896551724\tPurity: 0.0377401832973853\n",
            "Seed: 2\tF-Score:0.07862068965517241\tPurity: 0.04020053166301201\n",
            "Seed: 61\tF-Score:0.07448275862068965\tPurity: 0.03728585411723169\n",
            "Seed: 53\tF-Score:0.057931034482758624\tPurity: 0.043659453016565875\n",
            "Seed: 10\tF-Score:0.06482758620689655\tPurity: 0.03925704100838527\n",
            "Seed: 54\tF-Score:0.10482758620689656\tPurity: 0.04604471216436954\n",
            "Seed: 123\tF-Score:0.04551724137931035\tPurity: 0.03744759446977753\n",
            "Seed: 17\tF-Score:0.08965517241379309\tPurity: 0.03470339982537273\n",
            "Seed: 21\tF-Score:0.08\tPurity: 0.040096346993453796\n",
            "Seed: 27\tF-Score:0.07724137931034483\tPurity: 0.04349288194460593\n",
            "Seed: 65\tF-Score:0.07586206896551724\tPurity: 0.0488155766749684\n",
            "Seed: 135\tF-Score:0.11172413793103449\tPurity: 0.04180956080250367\n",
            "Seed: 98\tF-Score:0.06620689655172414\tPurity: 0.039396516512702\n",
            "Seed: 69\tF-Score:0.04827586206896552\tPurity: 0.046297459269117006\n",
            "Seed: 147\tF-Score:0.09517241379310344\tPurity: 0.03789334058108355\n",
            "Seed: 110\tF-Score:0.04551724137931035\tPurity: 0.03715414510420421\n",
            "Seed: 36\tF-Score:0.08275862068965517\tPurity: 0.0380373293989639\n",
            "Seed: 140\tF-Score:0.05517241379310345\tPurity: 0.042980014930666895\n",
            "Seed: 104\tF-Score:0.09241379310344827\tPurity: 0.043879641965769794\n",
            "Seed: 106\tF-Score:0.0910344827586207\tPurity: 0.03695960111776381\n",
            "Seed: 92\tF-Score:0.06206896551724138\tPurity: 0.03794281309835939\n",
            "Seed: 13\tF-Score:0.07862068965517241\tPurity: 0.03641894179206096\n",
            "Seed: 18\tF-Score:0.07172413793103448\tPurity: 0.04171561234620978\n",
            "Seed: 44\tF-Score:0.037241379310344824\tPurity: 0.04436239349332862\n",
            "Seed: 143\tF-Score:0.07310344827586207\tPurity: 0.032562649665485145\n",
            "Seed: 67\tF-Score:0.08551724137931034\tPurity: 0.03904045018870872\n",
            "Seed: 133\tF-Score:0.08\tPurity: 0.041565052193157793\n",
            "Seed: 6\tF-Score:0.06896551724137931\tPurity: 0.04013491485044117\n",
            "Seed: 137\tF-Score:0.038620689655172416\tPurity: 0.040912557157854024\n",
            "Seed: 123\tF-Score:0.04551724137931035\tPurity: 0.03744759446977753\n",
            "Seed: 111\tF-Score:0.046896551724137925\tPurity: 0.043034173391555706\n",
            "Seed: 102\tF-Score:0.09793103448275864\tPurity: 0.0372178368547697\n",
            "Seed: 120\tF-Score:0.05103448275862069\tPurity: 0.04267596008386681\n",
            "Seed: 138\tF-Score:0.08413793103448276\tPurity: 0.04056582484764101\n",
            "Seed: 7\tF-Score:0.0703448275862069\tPurity: 0.03866530591880801\n",
            "Seed: 39\tF-Score:0.0993103448275862\tPurity: 0.034255877686393314\n",
            "Seed: 106\tF-Score:0.0910344827586207\tPurity: 0.03695960111776381\n",
            "Seed: 115\tF-Score:0.05655172413793103\tPurity: 0.0384532148228569\n",
            "Seed: 95\tF-Score:0.07586206896551724\tPurity: 0.0377401832973853\n",
            "Seed: 78\tF-Score:0.0496551724137931\tPurity: 0.037348059748015784\n",
            "Seed: 3\tF-Score:0.08137931034482758\tPurity: 0.04168762594642023\n",
            "Seed: 83\tF-Score:0.08\tPurity: 0.03946567145089875\n",
            "Seed: 111\tF-Score:0.046896551724137925\tPurity: 0.043034173391555706\n",
            "Seed: 58\tF-Score:0.03310344827586207\tPurity: 0.0368241852039909\n",
            "Seed: 33\tF-Score:0.08551724137931034\tPurity: 0.045380213840255534\n",
            "Seed: 3\tF-Score:0.08137931034482758\tPurity: 0.04168762594642023\n",
            "Seed: 128\tF-Score:0.08275862068965517\tPurity: 0.037651470672364545\n",
            "Seed: 138\tF-Score:0.08413793103448276\tPurity: 0.04056582484764101\n",
            "Seed: 57\tF-Score:0.08827586206896551\tPurity: 0.04047244318333482\n",
            "Seed: 113\tF-Score:0.06206896551724138\tPurity: 0.038798668439578035\n",
            "Seed: 113\tF-Score:0.06206896551724138\tPurity: 0.038798668439578035\n",
            "Seed: 57\tF-Score:0.08827586206896551\tPurity: 0.04047244318333482\n",
            "Seed: 57\tF-Score:0.08827586206896551\tPurity: 0.04047244318333482\n",
            "Seed: 69\tF-Score:0.04827586206896552\tPurity: 0.046297459269117006\n",
            "Seed: 82\tF-Score:0.07724137931034483\tPurity: 0.04572314303498272\n",
            "Seed: 7\tF-Score:0.0703448275862069\tPurity: 0.03866530591880801\n",
            "Seed: 9\tF-Score:0.08137931034482758\tPurity: 0.04231854659601289\n",
            "Seed: 120\tF-Score:0.05103448275862069\tPurity: 0.04267596008386681\n",
            "Seed: 42\tF-Score:0.0993103448275862\tPurity: 0.03577264364268924\n",
            "Seed: 124\tF-Score:0.08\tPurity: 0.0365226607974619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XfpgCymh9T6",
        "outputId": "6fc07edc-926f-4683-f281-02b192040985"
      },
      "source": [
        "print(mf,'\\n', mp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: 135\tF-Score:0.11172413793103449\tPurity: 0.04180956080250367 \n",
            " Seed: 65\tF-Score:0.07586206896551724\tPurity: 0.0488155766749684\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}